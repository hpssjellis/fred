<!DOCTYPE html>
<html>
<head>
  <title>WebLLM Text to HTML</title>
</head>
<body>

  <script type="module">
    import { CreateMLCEngine, MLCEngine } from "https://esm.run/@mlc-ai/web-llm";

    // Declare the variable globally
    let engineInstance;

    // Initialize with a progress callback
    const initProgressCallback = (progress) => {
      console.log("Model loading progress:", progress);
      document.getElementById('htmlOutput').innerHTML = `Model loading progress: ${progress.text}`;
    };

    // Using CreateMLCEngine
    const engine = await CreateMLCEngine("Llama-3.2-1B-Instruct-q4f16_1-MLC", { initProgressCallback });
    /*
- **Llama**: Llama 3, Llama 2, Hermes-2-Pro-Llama-3
- **Phi**: Phi 3, Phi 2, Phi 1.5
- **Gemma**: Gemma-2B
- **Mistral**: Mistral-7B-v0.3, Hermes-2-Pro-Mistral-7B, NeuralHermes-2.5-Mistral-7B, OpenHermes-2.5-Mistral-7B
- **Qwen (通义千问)**: Qwen2 0.5B, 1.5B, 7B
    */


  </script>

  <h1>WebLLM Text to HTML</h1>

  <textarea id="textInput" rows="10" cols="50" placeholder="Enter your text here, then click button after the model has loaded..."></textarea><br><br>

  <input type="button" value="Generate HTML" onclick="myLLM()">

  <div id="htmlOutput"></div>

  <script>
    async function myLLM() {
    // Direct instantiation
    engineInstance = new MLCEngine({ initProgressCallback });
    await engineInstance.reload("Llama-3.2-1B-Instruct-q4f16_1-MLC");

    const messages = [{ role: "user", content: "How does WebLLM use workers?" }];
    const reply = await engineInstance.chat.completions.create({ messages });
    console.log(reply.choices[0].message.content);
      document.getElementById('htmlOutput').innerHTML = reply.choices[0].message.content;
    }
  </script>

</body>
</html>
