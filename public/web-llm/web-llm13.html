<!DOCTYPE html>
<html>
<head>
  <title>WebLLM Text to HTML SPA</title>
  <style>
    body {
      font-family: Arial, sans-serif;
    }
    #loading {
      display: none;
    }
  </style>
</head>
<body>

  <h1>WebLLM Text to HTML</h1>

  <textarea id="textInput" rows="10" cols="50" placeholder="Enter your text here, then click button after the model has loaded..."></textarea><br><br>
  <input type="button" value="Generate HTML" onclick="myLLM()">
  <div id="htmlOutput"></div>
  <div id="loading">Loading model, please wait...</div>
  <div id="modelList"></div>

  <script type="module">
    import { CreateMLCEngine, MLCEngine, prebuiltAppConfig } from "https://esm.run/@mlc-ai/web-llm";

    let engineInstance;

    const initProgressCallback = (progress) => {
      console.log("Model loading progress:", progress);
      document.getElementById('loading').style.display = 'block';
      document.getElementById('loading').innerHTML = `Model loading progress: ${progress.text}`;
    };

    const loadModel = async () => {
      const engine = await CreateMLCEngine("Llama-3.2-1B-Instruct-q4f16_1-MLC", { initProgressCallback });
      engineInstance = new MLCEngine({ initProgressCallback });
      await engineInstance.reload("Llama-3.2-1B-Instruct-q4f16_1-MLC");
      document.getElementById('loading').style.display = 'none';

      // Display available models
      const availableModels = prebuiltAppConfig.model_list.map((m) => m.model_id);
      document.getElementById('modelList').innerHTML = `<h2>Available Models:</h2><ul>${availableModels.map(model => `<li>${model}</li>`).join('')}</ul>`;
    };

    loadModel();

    window.myLLM = async function() {
      const messages = [{ role: 'user', content: document.getElementById('textInput').value }];
      const reply = await engineInstance.chat.completions.create({ messages });
      console.log(reply.choices[0].message.content);
      document.getElementById('htmlOutput').innerHTML = reply.choices[0].message.content;
    };
  </script>

</body>
</html>
